---
title: "Assignment #7::Dataset analysis preliminary work"
author: "Mert Efe","Burkan Bereketoglu"
date: "30 12 2021"
output: 
  pdf_document:
    includes:
      in_header: header.tex
---

# Introduction

I found the [\textcolor{blue}{Crime in Berlin, 2012 - 2019}](https://www.kaggle.com/danilzyryanov/crime-in-berlin-2012-2019) Dataset on Kaggle. This dataset consists of 1200 rows with 20 columns, where 4 columns are categorical and the rest numeric. The numeric features are occurrences how often a specific crime happend. 

# Libraries 

```{r attr.source='.numberLines'}
library(clusterSim)
library(clValid)
library(factoextra, quietly = TRUE)
suppressPackageStartupMessages(library(dendextend))
library(cluster)
library(reshape2, quietly = TRUE)
library(janitor, quietly = TRUE)
library(MASS)
library(reticulate, quietly = TRUE)
use_python("usr/bin/python3")
library(tidyverse, quietly = TRUE)
library(Rtsne)
```

\newpage

# Assignment #2 Dataset Selection

## Preliminary Analysis

### Dataset structure and properties

We load the dataset as a tibble. Tibble is a data frame which is part of the *tidyverse* library. *read_csv* reads comma delimited files.

```{r attr.source='.numberLines'}
data_tbl <- read_csv("Berlin_crimes.csv", show_col_types = FALSE)
```

Now we plot first samples and the shape of the dataset. 

```{r attr.source='.numberLines'}
# First samples of the dataset
head(data_tbl)
# shape of dataset
dim(data_tbl)
```

The first 4 columns are our categorical ones. The first describes the year, the second in which district the crime happened. The third one the district region and the fourth the location where the crime was. The remaining columns are the crimes categorized in different criminal offenses. The dataset has 1200 rows and 20 columns. 

Lets check if the dataset is valid. Therefore our dataset has to fulfill following conditions:

* In general: The dataset has no na's. 
* 1st column: All values are between 2012 and 2019. So the datatyp do not need to be a double. We can cast it as an integer and then check for the condition.
* 2nd column: Berlin consists of 12 districts. To check if we have these 12 districts we can use the unique() function. Afterward we can compare the unique()-list with a self created list containing all districts. To compare these two sets we're using the setdiff()-function.

* 3rd column: It is the same approach like the second. The district codes are well defined. If we have a list with all codes we could calculate again the set difference. Therefore we need to find out these district codes. This dataset is also unprepared available at [\textcolor{blue}{Berlin.de/Polizei}](https://www.berlin.de/polizei/service/kriminalitaetsatlas/). It is an official website from Berlin. Assumed all data are correct there we can extract the district codes and compare it with our district codes, like I did it in for the 2nd column. Therefore I downloaded the dataset from the website and removed handly unnecessary rows and columns. The datatyp can also converted from double to integer. 
* 4th column: Same idea as before. I need to extract the Location column to calculate the setdiff(). 
* 5th column - 20th column: All values should be positive. Furthermore since it does not exist a half crime we can convert the datatyp as an integer. 

```{r attr.source='.numberLines'}

print("Amount of na's in dataset: ")
sum(is.na(data_tbl))

# Cast Year from Double to Integer
data_tbl <- mutate_at(data_tbl, vars(Year), as.integer)

# Cast Code from Double to Integer
data_tbl <- mutate_at(data_tbl, vars(Code), as.integer)

paste("1st column: Dimension had to be 0x20 to be correct: ")
dim(filter(data_tbl, data_tbl$Year < 2012 | data_tbl$Year > 2019))

districts = c("Mitte", "Friedrichshain-Kreuzberg", "Pankow", "Charlottenburg-Wilmersdorf", 
              "Spandau", "Steglitz-Zehlendorf", "Tempelhof-Schöneberg", 
              "Neukölln", "Treptow-Köpenick", "Marzahn-Hellersdorf", 
              "Lichtenberg", "Reinickendorf")

print("2nd column: The set difference should be 0 to be correct.") 
lengths(setdiff(districts, unique(data_tbl$District)))

compare_data = mutate_at(read_csv("compare_data.csv", show_col_types = FALSE), vars(Code), as.integer)

print("3rd column: The set difference should be 0 to be correct.")
lengths(setdiff(compare_data$Code, unique(data_tbl$Code)))

print("4th column: The set difference should be 0 to be correct.")
lengths(setdiff(compare_data$Location, unique(data_tbl$Location)))

# Check for negative values 
for (i in 5:20) {
  tmp <- filter(data_tbl, data_tbl[i] < 0)
  print(paste("Amount of negative values for column", i, "are: ", dim(tmp)[1]))
}

crimes = c("Robbery","Street_robbery","Injury","Agg_assault","Threat","Theft",
           "Car","From_car","Bike","Burglary","Fire","Arson","Damage",
           "Graffiti","Drugs","Local")

# Cast Column 5 - 20 from Double to Int 
for (i in 5:20) {
  data_tbl <- mutate_at(data_tbl, vars(crimes[i-4]), as.integer)
}

head(data_tbl)

```

### Correlation between crimes

Now we want to take a look if there is a correlation between some crimes. Therefore we calculate the correlation matrix for all crimes using Pearson Correlation and plot a heat map. 

```{r attr.source='.numberLines', fig.width=14, fig.height=14}

# Correlation Matrix

# Source: https://rpubs.com/pinkrpub/698401

cor_matrix <- melt(cor(data_tbl[5:20], method = "pearson"))
ggplot(cor_matrix, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  ggtitle("Heap Map of Correlation Matrix for all Crimes")

```

### Statistics for 2012 and 2019

Last we compare some statistics for crimes in 2012 and crimes in 2019. The displayed statistics are related to all district regions. 

```{r attr.source='.numberLines', fig.width=12, fig.height=12}

crime_2012 = filter(data_tbl, data_tbl$Year == 2012)
crime_2019 = filter(data_tbl, data_tbl$Year == 2019)

cat("Statistics for Crime in 2012\n")
# Convert Year and Code to character. Otherwise summmary()- will calculate Mean, Median, etc.
summary(mutate_at( mutate_at(crime_2012, vars(Year), as.character), vars(Code), as.character))
cat("\n")

# Calculate Standard Deviation

standard_deviation_2012 = c(1:16)
standard_deviation_2019 = c(1:16)

# Error occurs using: standard_deviation_2012[i] <- sd(crime_2012[i+4])
# Fix: https://www.statology.org/r-list-object-cannot-be-coerced-to-type-double/

for (i in 1:16) {
  standard_deviation_2012[i] <- sd(as.numeric(unlist(crime_2012[i+4])))
}

for (i in 1:16) {
  standard_deviation_2019[i] <- sd(as.numeric(unlist(crime_2019[i+4])))
}

standard_deviation_2012_tbl <- tibble(crimes, standard_deviation_2012)
standard_deviation_2019_tbl <- tibble(crimes, standard_deviation_2019)

print(standard_deviation_2012_tbl)
print(standard_deviation_2019_tbl)

```


```{r attr.source='.numberLines', fig.width=6, fig.height=6}

sum_crimes_2012 = c(1:16)

for (i in 1:16) {
  sum_crimes_2012[i] <- sum(crime_2012[i+4])
}

crime_tibble_total_2012 <- tibble(crimes, sum_crimes_2012)

# Source: https://bookdown.org/dli/rguide/pie-chart.html
ggplot(crime_tibble_total_2012, aes(x="", y=sum_crimes_2012, fill=reorder(crimes, -sum_crimes_2012))) + 
  geom_bar(stat="identity", color = "black") + 
  coord_polar("y") + 
  ggtitle("Total Crimes in Berlin 2012", sum(crime_tibble_total_2012$sum_crimes_2012)) +
  theme_void()

```


```{r attr.source='.numberLines', fig.width=6, fig.height=6}

sum_crimes_2019 = c(1:16)

for (i in 1:16) {
  sum_crimes_2019[i] <- sum(crime_2019[i+4])
}

crime_tibble_total_2019 <- tibble(crimes, sum_crimes_2019)

# Source: https://bookdown.org/dli/rguide/pie-chart.html
ggplot(crime_tibble_total_2019, aes(x="", y=sum_crimes_2019, fill=reorder(crimes, -sum_crimes_2019))) + 
  geom_bar(stat="identity", color = "black") + 
  coord_polar("y") + 
  ggtitle("Total Crimes in Berlin 2019", sum(crime_tibble_total_2019$sum_crimes_2019)) +
  theme_void()

```


I can observe from the pie chart that in 2012 "*Damage* was the 3rd most common crime but in 2019 it moved to the 4th place. Unfortunaltly the amount of *Injury* has increased. *Bike* switched his place with *From_car* and *Threat* moved from the 7th to the 8th place. *Burglary* moved from the 8th place to the 11th place. On the other hand *Drugs* increased from place 9 to the 7th place. *Graffiti* did not changed his position. *Agg_assault* also increased in 2019. It moved from place 11 to place 9. *Robbery* and *Car* switched their places. For the reamaing ones it did not changed anything. I can also observe that the total amount of crimes shrinked from 534475 to 520086. So in Berlin happend in 2019 14389 fewer crimes then in 2012. 

\newpage

# Assignment #4::Projection of Data

## Data Preprocessing 

All district regions are separated listed depending on their district. So it might be helpful for the task to summarize them. We're summarizing all crimes depending on year and district. The district codes and the location are not relevant since we want to summarize the crimes regarding the district. So we're throwing them out. The new dataset will be a 96x18 tibble. 

``` {python attr.source='.numberLines'}
import numpy as np 
import pandas as pd

data = np.array(pd.read_csv("Berlin_crimes.csv", sep=',', header=None))
new_data = np.zeros((97, 18), dtype=object)

acc = np.zeros((16))
index = 1

for i in range(1, data.shape[0]):
    if i+1 == 1201:
        acc = acc + data[i, 4:20].astype(float)  
        new_data[index, 0] = data[i, 0]
        new_data[index, 1] = data[i, 1]
        new_data[index, 2:18] = acc 
        acc = np.zeros((16))
    else: 
        # Same Year
        if data[i, 0] == data[i+1, 0]:
            # Same Distrinct
            if data[i, 1] == data[i+1, 1]:
                acc = acc + data[i, 4:20].astype(float)
            # Differenct Distrinct
            else:
                acc = acc + data[i, 4:20].astype(float)  
                new_data[index, 0] = data[i, 0]
                new_data[index, 1] = data[i, 1]
                new_data[index, 2:18] = acc 
                acc = np.zeros((16))
                index += 1
        # Different Year
        else: 
                acc = acc + data[i, 4:20].astype(float)  
                new_data[index, 0] = data[i, 0]
                new_data[index, 1] = data[i, 1]
                new_data[index, 2:18] = acc 
                acc = np.zeros((16))
                index += 1
# Header
new_data[0, 0:2] = data[0, 0:2]
new_data[0, 2:] = data[0, 4:]
np.savetxt("Berlin_crimes_compressed.csv", new_data, delimiter=",", fmt="%s")

```

```{r attr.source='.numberLines'}
data_tbl <- read_csv("Berlin_crimes_compressed.csv", show_col_types = FALSE)
```

## Principle Component Analysis 

We're centering and scaling our data to calculate with prcomp() the principal components [2]. The function returns 5 parameters [7]. The first *sdev* contains the standard deviations of the principal components [7]. *rotation* contains the eigenvectors [4]. *x* contains the principal component score for each sample [4]. *scale* and *center* are containing the values with which PCA is scaled and centered [7]. \newline
With the summary()-function [2] we can observe that our first principal component covers 67.8% of the variance and the second 11.1%. So, using the first and second principal component, we cover 78.9% of the variance with them. To visualize it as a plot I created a Scree Plot using following References [1,5]. Also I plotted the data in the 2D space. 

```{r attr.source='.numberLines',  fig.width=14, fig.height=6}
pca_data_tbl <- prcomp(data_tbl[3:18], center = TRUE, scale = TRUE)
summary(pca_data_tbl)

# Creating a Scree Plot 
# Ref [5]
pca.var <- pca_data_tbl$sdev^2
prop <- pca.var/sum(pca.var)
# Ref [1]
barplot(prop, 
        ylim = c(0,0.7), 
        main = "Scree Plot", 
        xlab = "Principal Components", 
        ylab = "Proportion of Variance")

# Creating 2PC Plot
ggplot(as_tibble(pca_data_tbl$x), aes(x = PC1, y = PC2)) + 
  geom_point() + 
  xlab("PC1 (67.8%)") + 
  ylab("PC2 (11.1%)") + 
  ggtitle("First Two Principal Components")

```

To get a better representation of the data, we can plot labels instead of points [1]. Therefore I generated a list, containing the labels by concatenating the Year and Districts. 

```{r attr.source='.numberLines', fig.width=14, fig.height=20}
# creating empty list
year_district = list()
# adding the labels to list 
for (i in (1:96)) {
   year_district = append(year_district, 
                          paste(data_tbl$Year[i],data_tbl$District[i], sep = "_")
                          )
  }
# create tibble
pca_year_district <- tibble("Year_District" = year_district, 
               "PC1" = pca_data_tbl$x[,1], 
               "PC2" = pca_data_tbl$x[,2])

ggplot(data = pca_year_district, aes(x = PC1, y = PC2, label = Year_District)) +
  geom_text() + 
  geom_label() + 
  ggtitle("PCA Plot with Labels")
```

## Multi Dimensional Scaling (MDS)

### Classical (Metric) Multidimensional Scaling

An other linear approach is classical multidimensional scaling, also known as principle coordinate analysis. The cmdscale()-function takes the data in a distance structure and the dimension to which we want to reduce it [7].

```{r attr.source='.numberLines', fig.width=14, fig.height=6}
euclidean_data_mds <- cmdscale(dist(data_tbl[3:18]), k = 2)
ids <- c(1:96)
euclidean_data_pco_tbl <- tibble("ID" = ids, 
                                 "Dim1" = euclidean_data_mds[,1], 
                                 "Dim2" = euclidean_data_mds[,2])

ggplot(euclidean_data_pco_tbl, aes(x = Dim1, y = Dim2)) + 
  geom_point() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("Classical (Metric) Multidimension Scaling, k=2, Euclidean Distance")

```

Again we're plotting for a better representation a labeled variant. But before, we're calculating again Classical Multidimension Scaling however with manhatttan as the distance function. 
 
```{r attr.source='.numberLines', fig.width=14, fig.height=6}
manhattan_data_mds <- cmdscale(dist(data_tbl[3:18], method = "manhattan"), k = 2)
manhattan_data_pco_tbl <- tibble("ID" = ids, 
                                 "Dim1" = manhattan_data_mds[,1], 
                                 "Dim2" = manhattan_data_mds[,2])

ggplot(manhattan_data_pco_tbl, aes(x = Dim1, y = Dim2)) + 
  geom_point() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("Classical (Metric) Multidimension Scaling, k=2, Manhatten Distance")
```

Now we're taking a look to the labeled plots. First for the Classical Multidimension Scaling Data with euclidean as the distance function. 

```{r attr.source='.numberLines', fig.width=14, fig.height=20}
# create tibble
euclidean_pco_year_district <- tibble("Year_District" = year_district, 
                            "PC1" = euclidean_data_mds[,1], "PC2" = euclidean_data_mds[,2])
ggplot(data = euclidean_pco_year_district, aes(x = PC1, y = PC2, label = Year_District)) +
  geom_text() + 
  geom_label() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("PCO Plot with Labels, dist_method=Euclidean")
```

Now we're doing the same for the data with manhattan as the distance function. 

```{r attr.source='.numberLines', fig.width=14, fig.height=20}
# create tibble
manhattan_pco_year_district <- tibble("Year_District" = year_district, 
                            "PC1" = manhattan_data_mds[,1], "PC2" = manhattan_data_mds[,2])
ggplot(data = manhattan_pco_year_district, aes(x = PC1, y = PC2, label = Year_District)) +
  geom_text() + 
  geom_label() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("PCO Plot with Labels, dist_method=Manhattan")
```

### Sammon's Non-Linear Mapping

My last used Multi Dimensional Scaling method is Sammon's Non-Linear Mapping. It's provided in the MASS-Library [7] and was one of the given function from the lecture slides. It's also a non-linear mapping method. The results should be interesting, since the other methods where linear mapping methods. Again we have the data in a distance structure and the wanted dimension as an argument [7]. First we're performing it again with euclidean as distance function then with manhattan. 

```{r attr.source='.numberLines', fig.width=14, fig.height=6}
euclidean_data_sammon <- sammon(dist(data_tbl[3:18], method = "euclidean"), k = 2)
euclidean_sammon_points_tbl_year_district <- tibble("Year_District" = year_district, 
                            "Dim1" = euclidean_data_sammon$points[,1], 
                            "Dim2"= euclidean_data_sammon$points[,2])
ggplot(data = euclidean_sammon_points_tbl_year_district, aes(x = Dim1, y = Dim2)) + 
  geom_point() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("Sammon's Non-Linear Mapping, Euclidean")
```

```{r attr.source='.numberLines', fig.width=14, fig.height=20}
ggplot(data = euclidean_sammon_points_tbl_year_district, 
       aes(x = Dim1, y = Dim2, label = Year_District)) +
  geom_text() + 
  geom_label() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("Sammon's Non-Linear Mapping Plot with Labels,Euclidean")
```

Now we're performing the sammon()-function with manhattan as the distance function.

```{r attr.source='.numberLines', fig.width=14, fig.height=6}
manhattan_data_sammon <- sammon(dist(data_tbl[3:18], method = "manhattan"), k = 2)
manhattan_sammon_points_tbl_year_district <- tibble("Year_District" = year_district, 
                            "Dim1" = manhattan_data_sammon$points[,1], 
                            "Dim2"= manhattan_data_sammon$points[,2])
ggplot(data = manhattan_sammon_points_tbl_year_district, aes(x = Dim1, y = Dim2)) + 
  geom_point() + 
  xlab("Dim1") + 
  ylab("Dim2") + 
  ggtitle("Sammon's Non-Linear Mapping, Manhattan")
```

## Analysis and Evaluation of Results, and Discussion

1. Principle Component Analysis: 
    - In the **First Two Principal Components-Plot** we can observe a cluster right and a cluster with widely spread points in the middle to the left. Actually the cluster with widely spread points in the middle to the left could be seperated into two clusters, otherwise there would be huge outliers. 
    - In the **PCA Plot with Labels** we can observe that all crimes happend in *Mitte* are on the left side, while crimes in *Spandau*, *Marzahn-Hellersdorf*, *Reinickendorf*, *Steglitz-Zehlendorf*, *Lichtenberg*, and *Treptow-Köpenick* are clustered together at the right side. In the middle *Friedrichshain-Kreuzberg*, *Neukölln*, *Tempelhof-Schöneberg*, *Charlottenburg-Wilmersdorf*, and *Pankow* appears, whether *Friedrichshain-Kreuzberg*, and *Neukölln* are more left spread and *Tempelhof-Schöneberg*, *Charlottenburg-Wilmersdorf*, and *Pankow* more right. 
2. Classical (Metric) Multidimensional Scaling: 
    - Euclidean: 
      - We can observe that the scatter plot **Classical (Metric) Multidimension Scaling, k=2, Euclidean Distance** has some similarities to **First Two Principal Components-Plot**. However, here we can directly observe 3 different clusters. But we also can observe that we don't have so many points on the left side anymore, while the points in the middle increased. To check it we're taking a look to **PCO Plot with Labels, dist_method=Euclidean** to find out which points moved.
      - Taking a look to **PCO Plot with Labels, dist_method=Euclidean** with regard to **PCA Plot with Labels** the biggest difference is resulting in the middle and the left side. *Friedrichshain-Kreuzberg* which could be assigned to a Cluster with *Mitte* before is now a part of the middle one. The points in the middle are more closely together and are more on the right side. 
    - Manhattan: 
      - Comparing **Classical (Metric) Multidimension Scaling, k=2, Manhattan Distance** with the **Classical (Metric) Multidimension Scaling, k=2, Euclidean Distance** we can observe slightly changes. Calculation with Manhattan delivers more closer lying points then with Euclidean, where the points were more widely spread. Nevertheless the amount of possible cluster did not change. 
      - I don't think that the labeled output **PCO Plot with Labels, dist_method=Manhattan** have differences compared to **PCO Plot with Labels, dist_method=Euclidean**, since the scatter plots **Classical (Metric) Multidimension Scaling, k=2, Euclidean Distance** and **Classical (Metric) Multidimension Scaling, k=2, Manhattan Distance** looked quite similar. Nevertheless we take a look for it. 
      - As assumed, the data within a possible cluster are the same. No changes happened therefore. So the only difference between the two versions of *Classical (Metric) Multidimensional Scaling* is that the points are more close together with manhattan for the distance function.
3. Sammon's Non-Linear Mapping:
    - Euclidean: 
      - The results from **Sammon's Non-Linear Mapping, Euclidean** has very similar to **Classical (Metric) Multidimension Scaling, k=2, Euclidean Distance**. The changes are very slightly. Some points moved a bit within the possible cluster. Again we have three well separable clusters. Compared to **Classical (Metric) Multidimension Scaling, k=2, Manhattan Distance** the results are also quite similar. The points are just more widely spread in **Sammon's Non-Linear Mapping, Euclidean**. 
      - Regarding **Sammon's Non-Linear Mapping Plot with Labels,Euclidean** and compared to **PCO Plot with Labels, dist_method=Euclidean** and **PCO Plot with Labels, dist_method=Manhattan** the points within a possible clusters did not changed. *Mitte* is still alone at the left side, while the right points are still closely together and the middle points are a bit spread. 
    - Manhattan:
      - The output from **Sammon's Non-Linear Mapping, Manhattan** is identical with the output from **Classical (Metric) Multidimension Scaling, k=2, Manhattan Distance**. That's interesting since they are two different approaches. I expected slightly changes like **Sammon's Non-Linear Mapping, Euclidean** to **Classical (Metric) Multidimension Scaling, k=2, Euclidean Distance**, but there are none. So everything what I said about **Classical (Metric) Multidimension Scaling, k=2, Manhattan Distance** also applies to **Sammon's Non-Linear Mapping, Manhattan**. 


## Conclusion 

All projecting methods delivers similar outputs for the 2D space. For every method we can figure out three possible clusters and for most time the same points belongs to a cluster. The District *Mitte* provides for every method the leftest points, plotted in 2D space. For every method the districts *Spandau*, *Marzahn-Hellersdorf*, *Reinickendorf*, *Steglitz-Zehlendorf*, *Lichtenberg*, and *Treptow-Köpenick* forms the rightest cluster and for the most time the districts *Friedrichshain-Kreuzberg*, *Neukölln*, *Tempelhof-Schöneberg*, *Charlottenburg-Wilmersdorf*, and *Pankow* forms the middle cluster. Based on this results it would be interesting to find out, why they belong to a cluster.

\newpage

# Assignment#5:: Data Clustering

## Hierarchical Cluster Analysis 

### Finding best linkage

To perform hierarchical clustering in R we use the agnes()-method, which is provided in the cluster-library. As in [9] described, agnes is an agglomerative hierarchial cluster algorithm, which means that we start with a single cluster and repeat the algorithm until we formed a big cluster which consists of all leafs [9]. This is exactly the same like the example from the lecture. In the lecture we learned 4 types of linkages. MIN, MAX, Group Average and Distance Between Centroids. Theoretically there was a fifth called Ward's Method, however we did not talked about it in detail, I will just use these 4 types of linkages. The agnes()-method returns also the agglomerative coefficient which describes how strong the clustering structure is [9]. Unfortunately agnes() does not include Distance Between Centroids linkage. As an alternative I could use hclust() since they perform similar [9], but hclust() does not provide the agglomerative coefficient. For this reason I have to ignore the Distance Between Centroids linkage. So, the best way to find the best linkage will be performing agnes() our all linkages and compare their agglomerative coefficient. Before performing agnes() I will scale the data, which could be useful to have them in a same space. 

```{r attr.source='.numberLines', fig.width=14, fig.height=8}

scaled_data <- scale(data_tbl[3:18])
# Ref [9]: Maximum = complete, Minimum = single, Average = average
linkages = c("complete", "single", "average") #, "centroid")
dict_linkage = c("MAX", "MIN", "Group Average")
iter = 1
# Ref. [9]
for (i in linkages) {
  hierarchial_cluster <- agnes(scaled_data, method = i)
  print(pltree(hierarchial_cluster, cex = 0.5, hang = -1, 
               main = paste("Dendrogram of Agnes with", dict_linkage[iter],"-Linkage"), 
               xlab = paste("agglomerative coefficient", hierarchial_cluster$ac)))
  iter <- iter + 1
}

```

### Perform clustering on best linkage

After obtaining the best linkage for hierarchical clustering I can built the clusters using cutree() [9]. Since I need to provide a k for the number of cluster, I will create a for loop to find the best solution. 
```{r attr.source='.numberLines', fig.width=3.5, fig.height=4}
# MAX Linkage
best_linkage_hier_cluster <- agnes(scaled_data, method = "complete")

for (i in c(1:10)) {
  # Ref. [9]
  tree_cluster <- cutree(best_linkage_hier_cluster, k = i)
  print(fviz_cluster(list(cluster = tree_cluster, data = data_tbl[3:18]), 
                     main = paste("Hierarchical Clustering with", i, "Clusters")))
}

```

### Plotting the Dendrogram with colored clusters for K={4,5}

Last step for hierarchical cluster I will plot the clustered tree. 

```{r attr.source='.numberLines', fig.width=14, fig.height=8}
#list_tree_cluster = list()
for (i in c(4:5)) {
  # Ref. [9]
  tree_cluster <- cutree(best_linkage_hier_cluster, k = i)
  print(plot(as.hclust(best_linkage_hier_cluster), cex = 0.5, hang = -1))
  rect.hclust(best_linkage_hier_cluster, k = i, border = 2:i+1)
  #list_tree_cluster = append(list_tree_cluster, list(tree_cluster))
}
```


## K-Means Clustering  

### Clustering the data

kmeans() is the function to perform the K-Means algorithm in R. It takes the data and how many centers k we want to have. Optional, it also can take nstart. nstart it the number of multiple initial configurations [8]. So in my case, also as described in [8], I take nstart = 25 to have 25 inital configuration, where best one is taken. To find out the best cluster I created again a for loop for 10 runs. The results will be plotted and we can analyse them. 

```{r attr.source='.numberLines', fig.width=3.5, fig.height=4}
# Reference [8]
kmeans_cluster_list = list()

for (i in c(1:10)) {
  kmeans_cluster <- kmeans(scaled_data, centers = i, nstart = 25)
  print(fviz_cluster(kmeans_cluster, data = data_tbl[3:18], 
                     main = paste("kMeans with", i, "Clusters")))
  cat("\n")
  # Since kMeans could give different results for 
  # the same K because centroids are placed randomly, I save them. 
  kmeans_cluster_list = append(kmeans_cluster_list, list(kmeans_cluster))
}

```

### Analysing Cluster k = 4

Since KMeans with k = 4 and Hierarchical Clustering with cutree k = 4 provides the exact same solution, I'm using the solutions from kMeans to plot the Districts colored by their clusters. 

```{r attr.source='.numberLines', fig.width=14, fig.height=20}

kmeans_cluster_4 <- kmeans_cluster_list[[4]]

for (i in c(1:4)) {
  print(paste("Elements in Cluster", i, ":", kmeans_cluster_4$size[i]))
}

# create tibble
pca_year_district_cluster_kmeans <- tibble("Year_District" = year_district, 
               "PC1" = pca_data_tbl$x[,1], 
               "PC2" = pca_data_tbl$x[,2],
               "Cluster" = kmeans_cluster_4$cluster)

ggplot(data = pca_year_district_cluster_kmeans, aes(x = PC1, y = PC2, 
                                                    label = Year_District, 
                                                    fill = Cluster)) +
  geom_text() + 
  geom_label() + 
  ggtitle("PCA Plot with Labels and Clusters")

```

### Finding optimal K  

To find the optimal k for kMeans, I will use the Elbow Method. The Elbow Method wants to minimize the total within-cluster sum of square [8]. fviz_nbclust() is a function which calculates the total within-cluster sum of square for different k sizes and visualize them in a plot [8]. To obtain the optimal k, it is necessary to look for the bend [8]. 

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# Reference [8]
fviz_nbclust(scaled_data, kmeans, method = "wss") 
```

## Analysis and Evaluation of Results, and Discussion

1. Hierarchical Clustering 
    - Finding best linkage: 
        - The Dendrogram with the best agglomerative coefficient is the MAX-linkage one. This means that this linkage type provides the best results for the cluster structuring. The worst agglomerative coefficient results with MIN-linkage. It's only round about 0.697, while MAX-linkage is 0.898. The average linkage has a agglomerative coefficient value of 0.836. So, I will continue to work with linkage type MAX. 
        - I also observe that the Height is different for each type. While MAX-linkage has the largest Height with round about 14, MIN-Linkage has only Height ~4. Average-Linkage has a Height of ~8. In [9] is written that a heigher Height describes more less similarity of the observations.
    - Perform clustering on best linkage:
        - First of all, analyzing the different scatterplots it is interesting to realize that i would intuitively cluster the data in a different way then hierarchical clustering did. Cluster Plots with 1,2,7,8,9, and 10 clusters are nice to see, but not very useful to work with, because the data are too much or to barely separated. So the interesting cluster plots are 3,4,5, and 6. The most Plot I like is the one with 4 and 5 Clusters. In my opinion 6 Clusters are still a bit to much separated and the Plot with 3 Clusters has a very big Cluster and two small ones. 
    - Plotting the Dendrogram with colored clusters for K={4,5}:
        - In this plots we can observe how the clusters are defined in the tree structure. 
2. K-Means
    - Clustering the data:
        - k = 3 is exatcly the same way how I would intuitively separate the data. The Clusters for k = {1,2} are to big. The data for the Clusters k = {6,7,8,9,10} are to much separated. In my opinion the best result provide k = {3,4,5}, however the centroids are better in k = {4,5}. For convenience I will take the cluster k = 4, so we have a direct comparison with Hierarchial Clustering. Taking a closer look to both plots I observed that KMeans with k = 4 and Hierarchical Clustering with cutree k = 4 provides the exact same solution. 
        - Printing out the size of each Cluster shows that we have two small clusters with the same size and 2 with big clusters with many elements where their difference is a half of one cluster. Merging Cluster 1,2, and 4 together would result the same cluster size as cluster 3. However if I take a look to kmeans with k = 2 the algorithm would merge the cluster in a different way. 
        - If I take the **PCA Plot with Labels** from the last assignment and fill them by their corresponing cluster, I'm able to nicely read which districts are in one Cluster. *Mitte* and *Friedrichhain-Kreuzberg* are the small clusters containing only themselves for every year. *Neukölln*, *Pankow*, *Tempelhof-Schöneberg*, and *Charlottenburg-Wilmersdorf* are in the same cluster with size 32. The biggest cluster is formed by *Lichtenberg*, *Treptow-Köpenick*, *Steglitz-Zehlendorf*, *Marzahn-Hellersdorf*, *Spandau*, and *Reinickendorf* with size 48. All Clusters contain every year for every district. It never happens that a district is assigned to a different cluster for a specific year. 
    - Finding optimal K: 
        - With the Elbow Method it is possible to find the optimal amount of clusters for the data. I took earlier k = 4 for conveniences reasons, but the plot shows that k = 4 is the optimal amount of cluster for the data, because there is the bend in the graph. 

## Conclusion 

Different clustering approaches delivers different results. Its interesting to see that some of my thoughts, how data could be clustered, were wrong. Nevertheless clustering are useful to find out which data belongs to each other. I could find out in this assignment that if I take k = 4, I got the exact same result for two different clustering approaches. Two of these 4 clusters were very small with 8  elements each, the middle one contained 32 and the biggest one 48 elements. Its also interesting to see how the cluster changes if k increases. 

\newpage

# Assignment#6: Evaluation and Validity of Clusters

I will perform two different methods for each external, internal, relativ criteria for the results of KMeans and Hierarchical Clustering.

## external criteria

Unfortunately the dataset I chose has no labels. So, I'm not able to perform external criteria. 

## internal criteria

### Rousseeuw's Silhouette - KMeans

index.S() is a method provided in the clusterSim library. As learnt in th lecture, it calculates the internal cluster quality index. The silhouette index should be as close as possible to 1. First I calculate the index for all 10 calculated clusters for KMeans, then for the hierarchical clustering. Regarding the Ellbow Method in KMeans, I thought k=4 is the best number of clusters for the dataset. 
It is not possible to calculate the silhouette index for k=1, since an error occurs. So, I start with k=2. 

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
silhouette_list <- list()
for (i in c(2:10)) {
  silhouette_list <- append(silhouette_list, 
                            tibble("Cluster" = kmeans_cluster_list[[i]]$cluster))
}

#calculate index
silhouette_indicies <- list()
numb_cluster <- list()
for (i in c(1:9)) {
  silhouette_indicies <- append(silhouette_indicies, index.S(dist(scaled_data), 
                                                             silhouette_list[[i]]))
  numb_cluster <- append(numb_cluster, i)
}

plot_index_tbl <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl <- add_row(plot_index_tbl, 
                            "Index" = silhouette_indicies[[i]], 
                            "Cluster" = numb_cluster[[i]])
}

plot_index_tbl <- slice(plot_index_tbl, c(2:10))
#create plot
ggplot(plot_index_tbl, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Rousseeuw's Silhouette, KMeans") 

```

### Rousseeuw's Silhouette - Hierarchical Clustering 

```{r attr.source='.numberLines', fig.width=4, fig.height=4}

best_linkage_hier_cluster <- agnes(scaled_data, method = "complete")

hierarchical_clustering_list <- list()

for (i in c(1:10)) {
  # Ref. [9]
  hierarchical_clustering_list <- 
    append(hierarchical_clustering_list, 
           tibble("Clusters" = cutree(best_linkage_hier_cluster, k = i)))
}

# create list which contains cluster assignment for data for all clusters, except k=1
silhouette_list_hc <- list()
for (i in c(2:10)) {
  silhouette_list_hc <- append(silhouette_list_hc, 
                               tibble("Cluster" = hierarchical_clustering_list[[i]]))
}

#calculate index
silhouette_indicies_hc <- list()
numb_cluster_hc <- list()
for (i in c(1:9)) {
  silhouette_indicies_hc <- append(silhouette_indicies_hc, index.S(dist(scaled_data), 
                                                             silhouette_list_hc[[i]]))
  numb_cluster_hc <- append(numb_cluster_hc, i)
}

plot_index_tbl_hc <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_hc <- add_row(plot_index_tbl_hc, 
                            "Index" = silhouette_indicies_hc[[i]], 
                            "Cluster" = numb_cluster_hc[[i]])
}

plot_index_tbl_hc <- slice(plot_index_tbl_hc, c(2:10))
#create plot
ggplot(plot_index_tbl_hc, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Rousseeuw's Silhouette, 
          Hierarchical Clustering ") 

```

### C-Index - KMeans

In the R-Documentation is written that the number of clusters k with minimizes the Index is the optimal amount of clusters. The code is exactly the same as above for Silhouette. Just some variables and the called function is different. I used the index.C() function in the clusterSim library. 

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
c_index <- list()
for (i in c(2:10)) {
  c_index <- append(c_index, 
                            tibble("Cluster" = kmeans_cluster_list[[i]]$cluster))
}

#calculate index
indicies_cindex <- list()
numb_cluster_cindex <- list()
for (i in c(1:9)) {
  indicies_cindex <- append(indicies_cindex, index.C(dist(scaled_data), 
                                                             c_index[[i]]))
  numb_cluster_cindex <- append(numb_cluster_cindex, i)
}

plot_index_tbl_cindex <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_cindex <- add_row(plot_index_tbl_cindex, 
                            "Index" = indicies_cindex[[i]], 
                            "Cluster" = numb_cluster_cindex[[i]])
}

plot_index_tbl_cindex <- slice(plot_index_tbl_cindex, c(2:10))
#create plot
ggplot(plot_index_tbl_cindex, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("C-Index, KMeans") 

```

### C-Index - Hierarchical Clustering 

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
c_index_hc <- list()
for (i in c(2:10)) {
  c_index_hc <- append(c_index_hc, 
                            tibble("Cluster" = hierarchical_clustering_list[[i]]))
}

#calculate index
indicies_cindex_hc <- list()
numb_cluster_cindex_hc <- list()
for (i in c(1:9)) {
  indicies_cindex_hc <- append(indicies_cindex_hc, index.C(dist(scaled_data), 
                                                             c_index_hc[[i]]))
  numb_cluster_cindex_hc <- append(numb_cluster_cindex_hc, i)
}

plot_index_tbl_cindex_hc <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_cindex_hc <- add_row(plot_index_tbl_cindex_hc, 
                            "Index" = indicies_cindex_hc[[i]], 
                            "Cluster" = numb_cluster_cindex_hc[[i]])
}

plot_index_tbl_cindex_hc <- slice(plot_index_tbl_cindex_hc, c(2:10))
#create plot
ggplot(plot_index_tbl_cindex_hc, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("C-Index, Hierarchical Clustering ") 

```

## relative criteria

For calculating relative criteria I will use Davies-Bouldin's index from the clusterSim library and Dunn Index from the clValid library. Its again the same code as for calculating internal criteria, just with slightly changes.  

### Davies-Bouldin's index - KMeans

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
db_index <- list()
for (i in c(2:10)) {
  db_index <- append(db_index, 
                            tibble("Cluster" = kmeans_cluster_list[[i]]$cluster))
}

#calculate index
indicies_dbindex <- list()
numb_cluster_dbindex <- list()
for (i in c(1:9)) {
  indicies_dbindex <- append(indicies_dbindex, index.DB(scaled_data, db_index[[i]])$DB)
  numb_cluster_dbindex <- append(numb_cluster_dbindex, i)
}

plot_index_tbl_dbindex <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_dbindex <- add_row(plot_index_tbl_dbindex, 
                            "Index" = indicies_dbindex[[i]], 
                            "Cluster" = numb_cluster_dbindex[[i]])
}

plot_index_tbl_dbindex <- slice(plot_index_tbl_dbindex, c(2:10))
#create plot
ggplot(plot_index_tbl_dbindex, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Davies-Bouldin's index, KMeans") 

```

### Davies-Bouldin's index - Hierarchical Clustering

Regarding the R Documentation for index.DB(), the optimal amount of clusters will be the one which minimize the Davies-Bouldin's index. 

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
db_index_hc <- list()
for (i in c(2:10)) {
  db_index_hc <- append(db_index_hc, 
                            tibble("Cluster" = hierarchical_clustering_list[[i]]))
}

#calculate index
indicies_db_hc <- list()
numb_cluster_db_hc <- list()
for (i in c(1:9)) {
  indicies_db_hc <- append(indicies_db_hc, index.DB(scaled_data, db_index_hc[[i]])$DB,)
  numb_cluster_db_hc <- append(numb_cluster_db_hc, i)
}

plot_index_tbl_db_hc <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_db_hc <- add_row(plot_index_tbl_db_hc, 
                            "Index" = indicies_db_hc[[i]], 
                            "Cluster" = numb_cluster_db_hc[[i]])
}

plot_index_tbl_db_hc <- slice(plot_index_tbl_db_hc, c(2:10))
#create plot
ggplot(plot_index_tbl_db_hc, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Davies-Bouldin's index, 
          Hierarchical Clustering ") 

```

### Dunn Index - KMeans

Regarding the R Documentation for dunn(), the optimal amount of clusters will be the one which maximizes the Dunn Index.

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
dunn_index <- list()
for (i in c(2:10)) {
  dunn_index <- append(dunn_index, 
                            tibble("Cluster" = kmeans_cluster_list[[i]]$cluster))
}

#calculate index
indicies_dunn_index <- list()
numb_cluster_dunn_index <- list()
for (i in c(1:9)) {
  indicies_dunn_index <- append(indicies_dunn_index, dunn(dist(scaled_data), 
                                                          dunn_index[[i]]))
  numb_cluster_dunn_index <- append(numb_cluster_dunn_index, i)
}

plot_index_tbl_dunn_index <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_dunn_index <- add_row(plot_index_tbl_dunn_index, 
                            "Index" = indicies_dunn_index[[i]], 
                            "Cluster" = numb_cluster_dunn_index[[i]])
}

plot_index_tbl_dunn_index <- slice(plot_index_tbl_dunn_index, c(2:10))
#create plot
ggplot(plot_index_tbl_dunn_index, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Dunn Index, KMeans") 

```

### Dunn Index - Hierarchical Clustering

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
dunn_index_hc <- list()
for (i in c(2:10)) {
  dunn_index_hc <- append(dunn_index_hc, 
                            tibble("Cluster" = kmeans_cluster_list[[i]]$cluster))
}

#calculate index
indicies_dunn_index_hc <- list()
numb_cluster_dunn_index_hc <- list()
for (i in c(1:9)) {
  indicies_dunn_index_hc <- append(indicies_dunn_index_hc, dunn(dist(scaled_data), 
                                                                dunn_index_hc[[i]]))
  numb_cluster_dunn_index_hc <- append(numb_cluster_dunn_index_hc, i)
}

plot_index_tbl_dunn_index_hc <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_dunn_index_hc <- add_row(plot_index_tbl_dunn_index_hc, 
                            "Index" = indicies_dunn_index_hc[[i]], 
                            "Cluster" = numb_cluster_dunn_index_hc[[i]])
}

plot_index_tbl_dunn_index_hc <- slice(plot_index_tbl_dunn_index_hc, c(2:10))
#create plot
ggplot(plot_index_tbl_dunn_index_hc, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Dunn Index, 
          Hierarchical Clustering") 
```

## Analysis and Evaluation of Results, and Discussion

- internal criteria: 
    - Rousseeuw's Silhouette - KMeans:
        - Regarding the *Rousseeuw's Silhouette, KMeans* Plot I can observe that the best silhouette index is given for k = 2, followed by k = 4. So, compared to the Ellbow Method last Assignment, Rousseeuw's Silhouette shows that the optimal number of clusters are 2 instead of 4. However, 4 Clusters are also a good amount to handle with, since its the second best result. 
    - Rousseeuw's Silhouette - Hierarchical Clustering:
            - I thought just by taking a look to the Hierarchical Clustering plots, that k = 4 and k = 5 provides the best solutions. However, after calculating the Rousseeuw's Silhouette I can observe, that the best index is again given by k = 2, followed by k = 3. K = 4 and k = 5 are at the third and fourth place. Its also interesting to see that both KMeans and Hierarchical Clustering provides their best Rousseeuw's Silhouette for k = 2. 
    - C-Index - KMeans:
        - For the C-Index the optimal amount of clusters is given for k = 6 followed by k = 8. Compared with Rousseeuw's Silhouette it is interesting to see that for Rousseeuw's Silhouette was k = 6 the worst amount of clusters. 
    - C-Index - Hierarchical Clustering:  
        - The optimal amount of clusters for Hierarchical Clustering is k = 7, followed by k = 6. Again, comparing to Rousseeuw's Silhouette the results are completely different. 
- relative criteria: 
    - Davies-Bouldin's index - KMeans:
        - The Davies-Bouldin's index have regarded to the first two optimal amount of clusters the same result as Rousseeuw's Silhouette. k = 2 is the optimal one, followed by k = 4.
    - Davies-Bouldin's index - Hierarchical Clustering:
        - Again, regarded the first two optimal amount of clusters the same result as Rousseeuw's Silhouette. k = 2 is the optimal amount of clusters, followed by k = 3. 
    - Dunn Index - KMeans: 
        - For the Dunn Index the optimal cluster is k = 4 followed by k = 6. Its interesting to see that regarding my thoughts the best cluster size would be k = 4 matches for the Dunn index. Compared with Rousseeuw's Silhouette k = 4 was the second optimal result, however k = 6 was the worst and for the Dunn Index it is the second best cluster size. 
    - Dunn Index - Hierarchical Clustering:
        - Like for KMeans, the optimal cluster is k = 4 followed by k = 6.  
        
        
## Conclusion 

As for different clustering approaches, different criteria methods don't provide the same results. Regarding my thoughts for the last assignment that k = 4 will have the optimal cluster size, I was not completely wrong. I learnt in this assignment and can also conclude, that just by oberserving the plots for different clusters, it is not possible to find the best one. As an example I said for KMeans k = 6 that the data is too much separated, however for the C-Index it was the optimal solution. Also was k = 2 for both clustering apporaches often one of the optimal solutions which I did not expected. 

\newpage



\newpage


# t-SNE (Student t-Distributed Stochastic Neighbor Embedding)


## What is tSNE?


  tSNE is a technique in machine learning that is non-linear and can be said as a really useful dimensionality reduction method, which will allow the user to visualize data embedded in lower dimensions such as in 2-D, for to see patterns and trends in the data set. However, tSNE has some parts that newbies need to learn beforehand to use this technique efficiently such as the choice of hyperparameter tuning. tSNE due to it's hyperparameters have sensitivity issues, the parameter we are now looking for is the perplexity. Hyperparameters are the parameters that are controlled before the procedure occurs in the process. So one can think it as the amount of Xanthan gum you put which will determine the thickness and stickyness of the candy you'll make.

  Another important thing to add about the perplexity is that higher numbers in the perplexity will give us some perspectives on the global structure of the data set. Since in most cases our goal is to see some clusters that have local meanings. Which corresponds to similarities inside that cluster rather than dissimilarities between clusters created with tSNE. 

## Visualization and Selection

### Initial Dimensions of tSNE


  Here in this part we will focus on how to tune our hyperparameters such as the initial number of dimensions, you can reduce it linearly by PCA or else or you can by AutoEncoder or by another technique reduce non-linearly.

  We in past assignments found the PCA values, so we don't need to find them again, but we do need to find the value that is optimal to pick the best initial dimensions.

```{r, echo = TRUE}

expl_var <- pca_data_tbl$sdev^2/sum(pca_data_tbl$sdev^2)

N_perm <- 10
expl_var_perm <- matrix(NA, ncol = length(pca_data_tbl$sdev), nrow = N_perm)


for(k in 1:N_perm){
  expr_perm <- apply(data_tbl[3:18],2,sample)
  PC_perm <- prcomp(t(log10(expr_perm+1)), center=TRUE, scale=FALSE)
  expl_var_perm[k,] <- PC_perm$sdev^2/sum(PC_perm$sdev^2)
  }

plot(expl_var[1:16]~seq(1:16), ylab="EXPLAINED VARIANCE",
     col="darkgreen", type='o', xlab="PRINCIPAL COMPONENTS")
lines(colMeans(expl_var_perm)[1:16]~seq(1:16),col="red")
legend("topright", c("Explained by PCS", "Explained by chance"),
       fill=c("darkgreen","red"), inset=0.02)

pval <- apply(t(expl_var_perm) >= expl_var,1,sum) / N_perm

pval[1] <- pval[1]-1 # first element 
pval[16] <- pval[16] +1 # last element


plot(pval[1:16]~seq(1:16),col="darkred",type='o',
     xlab="PRINCIPAL COMPONENTS",ylab="PVALUE")
optPC<-head(which(pval>=0.05),1)-1
mtext(paste0("OPTIMAL NUMBER OF PRINCIPAL COMPONENTS = ",optPC))



```


  Here we see that our Optimal Number of Principal Component before the noise zone is 10. So, we should only take 10 initial dimensions. We manually needed to adjust the first and last element in the system to have the significance plot as intended. It is unreasonable to have noise by the first component then not to have on more dimensions also after entering the noise zone by permuted variance by PCA, there is no returning back. Those errors are deleted.
  

Now since we selected our optimal initial dims let's look at the optimal perplexity.


### Perplexity selection of tSNE

  Perplexity as one can mention is to be said that the most complicated of all hyperparameters of tSNE, and even the same mentioned by the author of tSNE, van der Maaten. Most of the time perplexities are in range between 5 to 50. The question of what about big data, for big data should we also need to have a perplexity in the range described or can we have bigger perplexities, the author says to us that denser/larger data set requires larger perplexities so it can be bigger than that.

  Optimally it will be below square root of the cell number/row number. Our function Rtsne can plot perplexities from 3 to nrow/3. So we can't go beyond 32 for our case. We used our compressed district table which is used in the rest of the studies and in k-medoids also in tSNE for convenience.


```{r attr.source='.numberLines', fig.width=4, fig.height=4, echo=TRUE}
library("Rtsne")

expr <- data_tbl[3:18]
N_cells<-dim(expr)[2]

par(mfrow=c(3,3))
perp_range<-vector(length=9)
perp_range[1]<-3; perp_range[9] <- (N_cells/3)-1

optPerp <- round(sqrt(N_cells),0); perp_step<-(optPerp-3)/4

for(s in 2:8){
  perp_range[s]<-perp_range[s-1]+perp_step
  }

for(j in round(perp_range,0)){
  tsne_perp_iter<-Rtsne(t(log10(expr+1)),perplexity=j,
                        initial_dims=optPC,max_iter=1000)
  plot(tsne_perp_iter$Y,col="blue",xlab="tSNE1",ylab="tSNE2",cex=0.5)
  mtext(paste0("perplexity = ",j))
}

```

  Perplexity, in a logarithmic scale has a dependence that is nearly linear with a coefficient of 1/2 to the number of cells if mathematically deriven. One can derive this by applying the above function to many many data set. Kullback-Leibler (KL) divergence is what actually tSNE based on when it comes to minimization. KL is also a function that if other parameters then perplexity fixed, decreases monotonically.

  But, can one use KL divergence to find optimum perplexity directly from it? Unfortunately, this is not what can be used, but many runs can give us the desired outcome rather than finding a value directly from KL as we did in the code. Since Kl behaves as 1/perp and we have perp/n, the normalized score will give us that number of cell in squared root is approximately perplexity number, but as we said it is not appropriate to use this Kl divergence derivation, it is just for us to develop an intuitive number.

  Lastly, now we will have the to find the optimal number of iterations.
  

### Tuning the number of iterations of tSNE

  Some can say that the best approach is always to iterate more and more and more, but this is not feasible. If one have a very big data set like ones that are in the CERN or FERMILAB or FED, even 1000 iterations can take days to have the end results. On the other hand, using few iterations won't work either since clusters may not be visible or data might be clumped in the center for few iterations. 
  
  So, what should we do to spend less time to get necessary amount of separation for understanding the similarities. So for that let's look at several iterations up to 1000 and pick the optimal.
  
  Let's first call our optimal dimensions and perplexity.
  

```{r, echo = TRUE}
paste("this is optimal initial dimensions.",optPC)
paste("this is optimal perplexity.",optPerp)

par(mfrow=c(3,3))

for(i in c(5, 10, 20, 50, 100, 200, 300, 500, 1000)){
	tsne <- Rtsne(t(log10(expr+1)), initial_dims=optPC,
		      perplexity=optPerp, max_iter=i)
	plot(tsne$Y, col="blue", xlab="tSNE1", ylab="tSNE2", cex=0.5)
	mtext(paste0("max_iter = ", i))
	}

```

  As from the plots it can be seen that up to 500 iterations separation is not well seen so we should pick either 500 or 1000 from our observation. Since there is not that many observation points we can just pick 500.
  
  Hence lastly plot the best tSNE result and validate it with shepard diagram.

 
  
### Plot
  
   Here we will finish with the tSNE plot that we found as its hyperparameters tuned & plotted.
  
```{r, echo=TRUE}
tSNE_Data = Rtsne(t(log10(expr+1)),initial_dims=optPC,perplexity=optPerp, max_iter=500)

df_tsne = data.frame(X = tSNE_Data$Y[,1],
                    Y = tSNE_Data$Y[,2],
                    Labels = factor(data_tbl$District))


    
plot_tsne = ggplot(data = df_tsne, aes(x = X ,y = Y, col = Labels)) +
  geom_point() +
  ggtitle("t-SNE Plot") +
  theme(plot.title = element_text(hjust = 0.5))
print(plot_tsne)

```

### Shepard Diagram Validation
 
  Here we continue with Shepard diagram for another validation that is in high demand by visualizing the data set in Shepard diagram to see the tSNE value whether passes the goodness of fit test.
  
  It will show us whether the tSNE that is produced by tuning is accurate visualization or not.
  
  
  
```{r, echo=TRUE}

dist_tcells_tSNE <- get_dist(select(df_tsne,
                           X, 
                           Y),
                           method = "euclidean")
dist_tcells <- get_dist(data_tbl[3:18], method = "spearman")

data_dist_tSNE <- tibble(org_distance = as.vector(dist_tcells),
                    tSNE_distance = as.vector(dist_tcells_tSNE))

ggplot(data_dist_tSNE, aes(x = org_distance, y = tSNE_distance)) +
       geom_hex(binwidth = c(0.05, 0.3)) +
       geom_smooth(color = "grey10", method = "loess", span = 0.5) +
       scale_fill_distiller(palette = "BrBG", direction = 1) +
       ggtitle(label = "Shepard plot for t-SNE",
               subtitle = "Original vs tSNE distances") +
      coord_fixed(1/(max(data_dist_tSNE$tSNE_distance)
                     - min(data_dist_tSNE$tSNE_distance))) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))


```
  
  Lastly the conclusion part of our data set with tSNE will come.

## Analysis and Evaluation of Results, and Discussion

- tSNE:

  -In the tSNE analysis, before getting the result and the end plot of the tSNE algorithm, we needed to tune the hyperparameters that needed to be determined by the user for the best result. 
  
  -First analysis was on the initial dimensions of the system that is going to be used for the tSNE and as an end result we received 10 as our optimal score. 
  
  -Later on we continued with the most important and popular hyperparameter of the tSNE which is Perplexity, as most of the time tSNE is used to see local similarities rather than any global difference or dissimilarities a finely tuned perplexity value will give us exactly what we are looking for. At the end we got perplexity score as 4.
  
  Then we tuned a third hyperparameter which is the number of iterations that is going to be conducted in the run of tSNE. We do want to tune it to lower the computation cost, so that the program end result won't come up days after we run it. For such low number of cells it is not that important, but it was still important to point that out.
  
  At the end we had our result which is several dots that made 3 clusters, which indicates that these districts are similar inside the same cluster. Since we have same district name for several different years, we have same color in different clusters since the similarity is not based on the same name but on theft, robbery or so it indicates that in different years different districts show similar pattern of crime rate with another district without the year indication.
  
  
## Conclusion 

  To conclude with the Shepard diagram result and the result of tSNE, we can say that the Shepard diagram is not that linear, but at some parts approximates to linearity still not linear though. The Shepard Diagram is a scatterplot that will give input proximities vs output distances for every pair of the items after the applied embedding/projection method, hence a linear correlation is the desired correlation.
  
  So the Shepard diagram result for our validation part indicates that the tuned tSNE result is not in perfect correlation. This indicates that ordering of the distances since not that linear is wrong in many cases.


\newpage




# Assignment#7:: Data Clustering

## K-Medoids 

### Visualizations for k = {1,...,10}
First, we plot the first 10 cluster results for k = {1,...,10}. Its the same approach like for the fifth Assignment. K-Medoids can be calculate with the pam()-function from the cluster library. 

```{r attr.source='.numberLines', fig.width=3.5, fig.height=4}
# Ref. [10] for Information how to calculate K Medoids
kmed_cluster_list = list()

for (i in c(1:10)) {
  kmed_cluster <- pam(scaled_data, i, metric = "euclidean")
  print(fviz_cluster(kmed_cluster, data = data_tbl[3:18], 
                     main = paste("kMedoids with", i, "Clusters")))
  cat("\n")
  kmed_cluster_list = append(kmed_cluster_list, list(kmed_cluster))
}
```

### Calculate optimal K 
Our first approach to calculate the optimal K is using the Elbow-Method regarding the total within sum of squares. The *bend* shows the optimal K. 
```{r attr.source='.numberLines', fig.width=4, fig.height=4}
fviz_nbclust(scaled_data, pam, method = "wss") 
```

The second approach is calculating the average silhouette which takes care for the intra and inter cluster distances. 
```{r attr.source='.numberLines', fig.width=4, fig.height=4}
fviz_nbclust(scaled_data, pam, method = "silhouette") 
```

```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# create list which contains cluster assignment for data for all clusters, except k=1
silhouette_list_kmed <- list()
for (i in c(2:10)) {
  silhouette_list_kmed <- append(silhouette_list_kmed, 
                            tibble("Cluster" = kmed_cluster_list[[i]]$cluster))
}

#calculate index
silhouette_indicies_kmed <- list()
numb_cluster_kmed <- list()
for (i in c(1:9)) {
  silhouette_indicies_kmed <- append(silhouette_indicies_kmed, index.S(dist(scaled_data), 
                                                             silhouette_list_kmed[[i]]))
  numb_cluster_kmed <- append(numb_cluster_kmed, i)
}

plot_index_tbl_kmed <- tibble("Index" = 0, "Cluster" = 0)
for (i in c(1:9)) {
  plot_index_tbl_kmed <- add_row(plot_index_tbl_kmed, 
                            "Index" = silhouette_indicies_kmed[[i]], 
                            "Cluster" = numb_cluster_kmed[[i]])
}

plot_index_tbl_kmed <- slice(plot_index_tbl_kmed, c(2:10))
#create plot
ggplot(plot_index_tbl_kmed, aes(x = Cluster, y = Index)) +
  geom_bar(stat="identity") + 
  theme(axis.ticks.x = element_blank(), 
        axis.text.x = element_blank()) +
  xlab("Number of Cluster, starting k=2") + 
  ggtitle("Rousseeuw's Silhouette, KMeans") 

```

Regarding both results, the optimal K is k = 4. 

### Plot optimal cluster
```{r attr.source='.numberLines', fig.width==3.5, fig.height=4}
fviz_cluster(kmed_cluster_list[[4]], data = data_tbl[3:18], 
             main = paste("K-Medoids with optimal K"))
```

```{r attr.source='.numberLines', fig.width=14, fig.height=20}

kmed_cluster_4 <- kmed_cluster_list[[4]]

# Since for k = 4, K-Medoids and K-Means have the same results
# We can use the variable kmeans_cluster_4 to access to the cluster sizes
for (i in c(1:4)) {
  print(paste("Elements in Cluster", i, ":", kmeans_cluster_4$size[i]))
}

# create tibble
pca_year_district_cluster_kmeans <- tibble("Year_District" = year_district, 
               "PC1" = pca_data_tbl$x[,1], 
               "PC2" = pca_data_tbl$x[,2],
               "Cluster" = kmed_cluster_4$cluster)

ggplot(data = pca_year_district_cluster_kmeans, aes(x = PC1, y = PC2, 
                                                    label = Year_District, 
                                                    fill = Cluster)) +
  geom_text() + 
  geom_label() + 
  ggtitle("PCA Plot with Labels and Clusters")

```


### Conclusion over all clustering approaches 
We can use the clValid()-function to find which of the algorithm provides the best results for a given K. It tries out k = {2,...,10} clusters for K-Means, Hierarchical with MAX-Linkage and K-Medoids and calculate the silhouette, the connectivitiy and the dunn index. 
```{r attr.source='.numberLines', fig.width=4, fig.height=4}
# Ref. [11, 12] 
library(clValid)
clmethods <- c("agnes", "kmeans", "pam")
internals <- clValid(scaled_data, nClust = 2:10, clMethods = clmethods, 
                     validation = "internal", method = "complete")
summary(internals)
```


## Analysis and Evaluation of Results, and Discussion

- K-Medoids:
    - Since our data is widely spread and K-Means is sensitive against outliers, we decided to use K-Medoids for the new clustering approach and compare it with K-Means. By comparing the all 10 plots for the differents K's its interesting to see, that every results is different expect for k = 4 which is exactly the same and the optimal one for K-Medoids. For K-Means is was the second best result for the silhouette criteria. 
    - To find the optimal K for K-Medoids we plot the total within sum of squares and tried to figure out, where the *bend* is. Then we calculated the average silhouette to verify our observation from the Elbow-Method. The optimal K for K-Medoids is k = 4. 
    - The sizes of the clusters are {8, 8, 32, 48}, so the clusters are unbalanced. 
    - Plotting data colored with the cluster assignments from K-Medoids we can observe, which districts are forming a cluster. *Mitte* and *Friedrichhain-Kreuzberg* are the small clusters containing only themselves for every year. *Neukölln*, *Pankow*, *Tempelhof-Schöneberg*, and *Charlottenburg-Wilmersdorf* are in the same cluster with size 32. The biggest cluster is formed by *Lichtenberg*, *Treptow-Köpenick*, *Steglitz-Zehlendorf*, *Marzahn-Hellersdorf*, *Spandau*, and *Reinickendorf* with size 48. All Clusters contain every year for every district. It never happens that a district is assigned to a different cluster for a specific year. 
    - Last we used clValid() to find out the best clustering algorithm for the dataset. The best performing one is Hierarchical Clustering with MAX-Linkage for k = {2,3}. The best connectivity and the best silhouette value is given for k = 2, however the best dunn index is provided for k = 3. 
   
    
## Conclusion 


## References 
- [1] StatQuest with Josh Starmer, "StatQuest: PCA in R", 27 11 2017. [Online]. Available: https://www.youtube.com/watch?v=0Jp4gsfOLMs. [Accessed 19 11 2021]
- [2] P. Nistrup, "Principal Component Analysis (PCA) 101, using R", 29.01.2019. [Online]. Available: https://towardsdatascience.com/principal-component-analysis-pca-101-using-r-361f4c53a9ff [Accessed 19 11 2021]
- [3] Dataset: https://www.kaggle.com/danilzyryanov/crime-in-berlin-2012-2019
- [4] Zach, "Principal Components Analysis in R: Step-by-Step Example", 01 12 2020. [Online]. Available: https://www.statology.org/principal-components-analysis-in-r/. [Accessed 19 11 2021]
- [5] Analytics Vidhya, "PCA: A Practical Guide to Principal Component Analysis in R & Python", 21 03 2016. [Online]. Available: https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/. [Accessed 19 11 2021]
- [6] user3498523, "add image in title page of rmarkdown pdf", 20 09 2015. [Online Forum]. Available: https://stackoverflow.com/questions/29389149/add-image-in-title-page-of-rmarkdown-pdf. [Accessed 23 11 2021]
- [7] R Documentation
- [8] "K-means Cluster Analysis". [Online]. Available: https://uc-r.github.io/kmeans_clustering [Accessed 02 12 2021]
- [9] "Hierarchical Cluster Analysis" [Online]. Available: https://uc-r.github.io/hc_clustering [Accessed 29 11 2021]
- [10] "K-Medoids in R: Algorithm and Practical Examples" [Online]. https://www.datanovia.com/en/lessons/k-medoids-in-r-algorithm-and-practical-examples/ [Accessed 27 12 2021]
- [11] "Package ‘clValid’", 14.02.2021. [Online]. Available: https://cran.r-project.org/web/packages/clValid/clValid.pdf [Accessed 27 12 2021]
- [12] "Choosing the Best Clustering Algorithms", [Online]. Available: https://www.datanovia.com/en/lessons/choosing-the-best-clustering-algorithms/ [Accessed 27 12 2021]
- [13] "How to tune hyperparameters of tSNE" [Online]. Available:
https://towardsdatascience.com/how-to-tune-hyperparameters-of-tsne-7c0596a18868 [Accessed 26 12 2021]
- [14] "Quick and easy t-SNE analysis in R", [Online]. Available:
https://www.r-bloggers.com/2019/05/quick-and-easy-t-sne-analysis-in-r/ [Accessed 27 12 2021]
- [15] "Levi Function Optimization with Simulated Annealing in R",[Video]. Available:
https://www.youtube.com/watch?v=p8Rk_riTPV0 [Accessed 27 12 2021]
- [16] "How to Use t-SNE Effectively" [Online] Available: https://distill.pub/2016/misread-tsne/
[Accessed 3 1 2022]
- [17] "Comparing t-SNE solutions using their Kullback-Leibler divergences" [Online] Available: https://stats.stackexchange.com/questions/248478/comparing-t-sne-solutions-using-their-kullback-leibler-divergences [Accessed 4 1 2022]