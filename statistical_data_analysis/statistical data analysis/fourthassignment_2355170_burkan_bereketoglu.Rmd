---
title: "Fourth Assignment - CENG574"
author: "Abdullah Burkan Bereketoglu"
date: "11/22/2021"
output:
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document:
    toc: yes
    toc_depth: 5
    number_sections: yes
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, out.height= '20%', out.width= '20%'}
knitr::include_graphics("E:/Tubitak/Abdullah_Burkan-Bereketoglu_ChE204-2020Fall.jpg")
```

# CENG 574 Assignment 4 - Projection of Data

## Introduction
  For week of 1/11/2021 - 7/11/2021 we are given the task of selecting data-set from one of the various platforms. In this project the data-set is taken from "[Kaggle](https://www.kaggle.com/)" platform. Name of the data-set is "[Loan Prediction Based on Customer Behavior](https://www.kaggle.com/subhamjain/loan-prediction-based-on-customer-behavior?select=Test+Data.csv)". The "metadata" of the data-set informs us that this data is collected for a Hackathon named "Univ.Ai". The data-set covers information on consumer loans all across India.[1]
  
  In the data-set a training observation of 252000 people and also a test sample that consists of 28000 people is provided with the necessary features, to analyze the test sample's each individuals risk_flag from the training set.
  
  Risk flag shows us whether an individual is defaulted on a loan or not which helps the banks to whether give or not give a loan to an individual next time. In training set data of people with different income, age, gender, marital status etc. given also with the Risk Flag data to train our model and then test the test data to understand whether a person didn't pay their loan or not.

## Preliminary Analysis
### Some information about the dataset
#### Reading the Data

Let's first load the data-set and look at what we got in the data-set.

```{r, echo=TRUE}
library(readr)

Loan_data_train <- read.csv("Training_Data.csv", header = TRUE, sep = ",")

Loan_data_test <- read.csv("Test_Data.csv", header = TRUE, sep = ",")

head(Loan_data_train) #looking at the head of the data to see the columns and some values of train.

head(Loan_data_test) # looking at the head of the data to see the columns and some values of test.
```
#### Some of the Basic Properties
We can also look at the column Names, how many columns are there and how many observations, which we can name as rows in the data-set.
```{r, echo=FALSE}
#Names of the features

names(Loan_data_train)

```

```{r, echo=TRUE, include=FALSE}
definitions <- c("Shows each individual observation given in the data-set, represented as integer.",
                 "Shows each individual Id's annual salary represented as integer.",
                 "Shows years lived 'till data is taken of the individual. Represented in integer values.",
                 "Professional experience level of the individuals given in years.Represented in integer values.",
                 "Professional Work of the individual.Represented in string values.",
                 "Shows whether the individual is married or not. (No data on separated, engaged etc.). In string form.",
                 "Shows whether individual owned or rented or neither of the home ownership. Represented in integer values.",
                 "Shows whether the individual owns a car or not. Represented in integer values.",
                 "Whether the individual is defaulted on a loan or not. Represented in integer values.",
                 "Years of experience in the current job is given of individual's.Represented in integer values.",
                 "Number of years in the current residence of the individual's. Represented in integer values.",
                 "City the individual currently resides in. Represented in string values.",
                 "State of residence of individual in data-set.Represented in string values.")
```
These column names are called features.

Let's cover what they represent in the data.
```{r, echo=TRUE, results='asis'}
library(knitr)

define_table <- cbind(colnames(Loan_data_train),definitions)
kable(define_table, col.names = c("Features","Definitions"))
```


```{r, echo=TRUE}
#Variable count or features of train
feature_loan_data_train = ncol(Loan_data_train)

#Variable count or features of test
feature_loan_data_test = ncol(Loan_data_test)

# Observation count of train
obs_loan_data_train = nrow(Loan_data_train)

# Observation count of test
obs_loan_data_test = nrow(Loan_data_test)

sprintf("Total observation count in the training data is %s and the number of features(columns) is %s.",
        obs_loan_data_train, feature_loan_data_train)
```
  Now, let's look at the test data feature names, counts, and sample count.
```{r, echo=TRUE}
sprintf("Total observation count in the test data is %s and the number of features(columns) is %s.",
        obs_loan_data_test, feature_loan_data_test)

colnames(Loan_data_test)
```

What makes this change???
We will now see the missing features with a slight error in the Test data.

```{r, echo=TRUE, warning=FALSE}

if (colnames(Loan_data_train) != colnames(Loan_data_test)) {
  missing_features <- c(colnames(Loan_data_train[colnames(Loan_data_train) != colnames(Loan_data_test)]))

}

length(missing_features) # how many missing or different features.
```

  In two data-set one can see that there are 2 differences between the feature count and that is the 
```{r, echo=FALSE, warning=FALSE}
missing_features
```
values. 

"Id" tag is shown as different feature, because the observation count is not equal in test and train data-set. So, by the end of analysis we end up with only "Risk_Flag" as our different feature.

Since, the data-set covers a column for the sorting, which is the "Risk_flag" that is not in test data is not a feature but the end result of the other features other than "Id".

Therefore we can conclude that we only have 11 features.

```{r, echo = TRUE}

print(Loan_data_train$Income[1:10])
```

  It is important to note that Income values given in the data-set is in Indian Rupee.

```{r, echo = TRUE}
plot(Loan_data_train[0:50,])

```

  A general plot is made to see all variables together plotted one by one with each other.
  
```{r, echo=TRUE, message=FALSE}

attach(Loan_data_train)

library(datasets)
library(tidyverse)
library(tibble)

comparison <- as_tibble(Loan_data_train %>% 
  select(Age, Income, Risk_Flag) %>% 
  arrange(desc(Age)))

summary(comparison)
cor(Age,Income)
cor(Risk_Flag,Age)

```

#### Determining The Unique Values of the Data  
##### Training Data  
In this part, we will look at the each individual unique value in each column for our training data.
```{r, echo = TRUE, message=FALSE}
library("dplyr") 
Loan_data_train %>% 
  select(-c("Id"))%>%
  summarise_all(n_distinct)

```
  Let's look at how many of the individuals are not paying their loans.
  
```{r, echo = TRUE, message=FALSE}

Loan_data_train %>% 
  group_by(Risk_Flag)%>%
  summarise(count=n())
```
  This gives us an idea that most of the individuals don't have a risk flag for a bank loan.

### Comparison of Data Variables

  Let's do comparison.
  
```{r, echo=TRUE, message=FALSE}

attach(Loan_data_train)

filtered_young_ages_risk_factor <- Loan_data_train %>%
  filter(Risk_Flag %in% c(0,1), Age < 50) %>%
  select((c(Age,Risk_Flag))) %>%
  arrange(desc(Age)) %>%
  group_by(Age) %>%
  summarize(mean = mean(Risk_Flag), TotalNumber= n())

head(filtered_young_ages_risk_factor)
mean(filtered_young_ages_risk_factor$mean)
```
  here we have for old people.
```{r, echo=TRUE, message=FALSE}

attach(Loan_data_train)

filtered_old_ages_risk_factor <- Loan_data_train %>%
  filter(Risk_Flag %in% c(0,1), Age > 50) %>%
  select((c(Age,Risk_Flag))) %>%
  arrange(desc(Age)) %>%
  group_by(Age) %>%
  summarize(mean = mean(Risk_Flag), TotalNumber= n())

head(filtered_old_ages_risk_factor)
mean(filtered_old_ages_risk_factor$mean)
```
  By this we can see young people's Risk flag means also mean of old people's Risk flag to understand whether there is a correlation between being old and not paying or the opposite.
  
```{r, echo=TRUE}
sprintf("Mean of Risk Flag for old people is %s and for young people is %s.",
        mean(filtered_old_ages_risk_factor$mean), mean(filtered_young_ages_risk_factor$mean))
```

## Methods and Code (PCA, MDS)
### Preprocessing the data
  Our data consists of numeric and non-numeric categorical data hence we need to figure out a way to turn these categorical data into numeric data that we can use.

```{r, echo=TRUE}
Loan_dat_train_x <-Loan_data_train
m_s <- unique(Loan_dat_train_x[c("Married.Single")])
c_o <- unique(Loan_dat_train_x[c("Car_Ownership")])
h_o <- unique(Loan_dat_train_x[c("House_Ownership")])
val <- 1


while (val <= length(Loan_dat_train_x$Married.Single)){
  Loan_dat_train_x$Married.Single[val] <- ifelse(Loan_data_train$Married.Single[val] == m_s[2,1],1,0)

  val = val + 1 
}

val <- 1

while (val <= length(Loan_dat_train_x$Car_Ownership)){
    Loan_dat_train_x$Car_Ownership[val] <- ifelse(Loan_data_train$Car_Ownership[val] == c_o[2,1],1,0)
    val = val + 1 
}

val <- 1

while (val <= length(Loan_dat_train_x$House_Ownership)){
  if(Loan_data_train$House_Ownership[val] == h_o[3,1]){
    Loan_dat_train_x$House_Ownership[val] <- 2
  } else if (Loan_data_train$House_Ownership[val] == h_o[1,1]){
    Loan_dat_train_x$House_Ownership[val] <- 1
  } else{
    Loan_dat_train_x$House_Ownership[val] <- 0
  }
  val = val + 1 
}

val <- 1


```
  In here Unique values of Married.Single, Car_Ownership, and House_ownership features are encoded.
One hot encoding on married.single and car ownership features applied, and label encoding is applied on house_ownership feature. 

  In Married.Single, married == 1 and single is selected as 0. In Car_Ownership feature we selected owning a car as 1 and not owning as 0. Lastly on house_ownership label encoding is done as having a house is 2 renting is 1 and not having or renting a house is 0.
  
  I believe that now we can use our PCA model much better.
  
### Principal Component Analysis

```{r, echo=TRUE}
library("dplyr") 
library("ggplot2")
Loan_data_train_numeric<- Loan_dat_train_x %>% 
  select(-c("Profession","CITY","STATE","Risk_Flag","Id"))

Loan_data_train_numeric<- lapply(Loan_data_train_numeric,as.numeric)

Loan_data_train_numeric <- data.frame(Loan_data_train_numeric)

scaled.small <- scale(Loan_data_train_numeric)
pca <- prcomp(scaled.small)

# plot pc1 and pc2
plot(pca$x[,1],pca$x[,2], main = "PC1 vs PC2", sub = "(pc of 1 and 2)",
     xlab="pc1",ylab="pc2")

screeplot(pca)
```
  
  In the plot of pc1 vs pc2 we see the standardized data vary a lot between the scattered zone.
  In the scree-plot we see that the variance is bigger than one for income principal component and that gives us the idea that it varies much and using mean is not useful for such component.
  
  Let's look at the pca variances from a point of view that is percent variation.

```{r, echo=TRUE}


## scree plot making
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

barplot(pca.var.per, main ="Scree Plot", xlab= "Principal Component", ylab = "Percent Variation")

```

  Here in the percent variation Scree Plot we see that pc1 is the most varying and its score is 20%.
  
  Let's also make multidimensional scaling analysis to compare and understand the data more.

### Multidimensional Scaling

```{r, echo=TRUE}
library("dplyr")
require(dplyr)
require(MASS)
Loan_data_train_mds_1 <- Loan_dat_train_x %>% 
  dplyr::select(-c("Profession","CITY","STATE","Risk_Flag"))

rownames(Loan_data_train_mds_1) = Loan_data_train_mds_1[,1]

Loan_data_train_mds_1 = Loan_data_train_mds_1[,-1]

Loan_data_train_mds_1=as.matrix(Loan_data_train_mds_1[,-1])
dconfig <- as.dist(t(head(Loan_data_train_mds_1,n=7)))


dconfig_tail <- as.dist(t(tail(Loan_data_train_mds_1, n=7)))

``` 
  For to make Classical (Metric) Multidimensional Scaling we needed to first make our data converted to matrix but there is a problem. Our data is not square matrix even though we can convert it to. Since there are only 7 features that we can do the analysis in and 252000 observations 252000x7 is nowhere near a square matrix. Hence I look at the transpose of head and transpose of tail to make the Metric MDS and understand. Any data between 5 and -5 shows 

  Here we will analyze our training data with Kruskal's Non-metric Multidimensional Scaling and Sammon's Non-Linear Mapping for analyzing similarities and dissimilarities in the dataset features.

```{r, echo=TRUE}
library("dplyr")
library(MASS)
require(dplyr)
require(MASS)


Loan_data_train_mds_2 <- Loan_dat_train_x %>% 
  dplyr::select(c("Id","Income","Age","Experience","House_Ownership","CURRENT_JOB_YRS","CURRENT_HOUSE_YRS"))

rownames(Loan_data_train_mds_2) = Loan_data_train_mds_2[,1]

Loan_data_train_mds_2 = Loan_data_train_mds_2[,-1]

Loan_data_train_mds_2 = as.matrix(Loan_data_train_mds_2[,-1])
dconfig_2 <- as.dist(t(head(Loan_data_train_mds_2,n=7)))


sammons <- sammon(dconfig_2, y = cmdscale(dconfig_2, 2), k = 2)


kruskal <- isoMDS(dconfig_2, k = 2)

```
  Here, in the MDSPlot[3], we can see the locations and similarities of features of the dataset. Sammons shows stress which is our goal to minimize it. it is the minimization of the mappings error in distances. It can be made by gradient descent too. A desired stress level and points would be belove 5 and in our 2D mapping both satisfy the condition for both kruskal and sammon.

## Results

  Here are the end results for to make things more clear.
  First Kruskal non-metric MDS [5].

```{r, echo=TRUE}

kruskal # For isoMDS Kruskals nonmetric MDS.

MDSPlot <- function(X)
{
  
  #creates an appropriate plot from an MDS object
  x = X$points[,1]
  y = X$points[,2]
  plot(X$points, pch=20, col = "purple",
       xlab = "First Dimension", xaxt ="n",
       ylab = "Second Dimension", yaxt = "n",
       main = "Perceptual Map for Loans", xlim= c(min(x)-5*sd(x),max(x)+.5*sd(x)),ylim = c(min(y)-5*sd(y),max(y)+5*sd(y)))
  text(X$points, labels = dimnames(X$points)[[1]],pos = c(1,2,3,4),cex =.75,col = c("black","blue","green","red"))
}

MDSPlot(kruskal)

```
  
  Second we have Sammon's Non-Linear Mapping [4].
  
```{r, echo = TRUE}
sammons # For sammon Sammons nonmetric MDS.
sammons$stress
```
  Here lastly Classical (Metric) Multidimensional Scaling which is actually Principal Coordinates Analysis (as mentioned in stats) [2].  
```{r, echo = TRUE}
# find the MDS solution

metric_mds = cmdscale(dconfig, k= 1)
metric_mds

metric_mds = cmdscale(dconfig,k=2)
metric_mds

metric_mds = cmdscale(dconfig, k=3)
metric_mds

# Also look from the tail of the data
metric_mds_tail = cmdscale(dconfig_tail, k = 1)
metric_mds_tail

metric_mds_tail = cmdscale(dconfig_tail, k = 2)
metric_mds_tail

metric_mds_tail = cmdscale(dconfig_tail, k = 3)
metric_mds_tail


```

## Analysis and Evaluation of Results, and Discussion

  Before writing anything about the analysis and evaluation of the results, one needs to know how to determine the appropriate minimum dimensions to map the data is about for sammons dissimilarity measures that are in between 5 and -5 and for its stress below 5 in our case its 0.03 which is great. For regular classic metric mds the values that pop-up needs to be in between 10 for some but mostly as usual between 5 and -5. These values show the probability of error being made and scores in 2 dimensions is sufficient for us.

  By analyzing our PCA and MDS analysis results, from all MDS models tried (Kruskal, Sammon, and Classic Metric) numeric features that are one-hot and label coded one can think of this dataset can be thought as a dataset that can be mapped to two dimensions without not many meaningful data loss. Also from the PCA which has more features and observations available due to not needing a square matrix, shows us the variance across the features and gives us the result that income is the most varying feature of the dataset with 20%. 
  
  What more can be thought of the data to be exploring the dataset with other types of comparison techniques in the future, since PCA is an exploratory tool that shows and involves observations on n-numeric variables that define pn-dimensional vectors that if there are p entities. With that we find eigenvectors and eigenvalues that help us to understand the matrix of the dataset. One can get more meaningful information of the covariance matrix of the dataset.
  
  One can use a MPCA technique and look for it's measures to understand, which now that I don't know whether will it work. Least squares and maximum likelihoods of each features than make another comparison with the techniques used. Lastly, I want to add that label encoding and one-hot encoding may made the data lose some of its meaning, also not being able to use some features due to not label encoding the states and city variables.
  

## Conclusion

We, by the conducted preliminary analysis of the "Univ.Ai" data-set on "Loan Prediction Based on Customer Behavior", learned about the observation-training size, different features that is used in the case to analyze risk to whether give bank loan to a person on the test-sample or not, with various characteristics of the features given in the data-set.

Later on, by the principal component analysis(PCA) and multidimensional scaling (MDS), we learned about similarities, variation, covariation, variability, dissimilarities of the data. Also which dimension of the scaling is a nice fit with some selection of the dataset.

As conclusion, we can deduce that there is not a big risk flag gap for elderly and youth, and also to add to the conclusion categorical Data that the ones that can be encoded to become numeric data must be encoded and the data and feature loss should be mentioned.

## References

[1] Subham Surana, "Loan Prediction Based on Customer Behavior."Aug. 2021, Accessed: Nov. 3, 2021. [Online]. Available: https://www.kaggle.com/subhamjain/loan-prediction-based-on-customer-behavior/metadata

[2]https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cmdscale.html

[3]Peter Rossi, "MDS in R" May. 2013, Accessed: Nov. 23, 2021. [Video]. Available: https://www.youtube.com/watch?v=cwE-zx3goBo

[4]https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/sammon.html

[5]https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/isoMDS.html

[6]Josh Starmer, "StatQuest: PCA in R" Nov. 2017, Accessed: Nov. 23, 2021. [Video] https://www.youtube.com/watch?v=0Jp4gsfOLMs